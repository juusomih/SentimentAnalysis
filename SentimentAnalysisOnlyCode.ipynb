{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import allfunctions as func\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from scipy.spatial.distance import cdist\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, GRU, Embedding, Dropout, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#required download for pos_tagger to work\n",
    "#nltk.download('averaged_perceptron_tagger') \n",
    "\n",
    "max_features = 10000 # (Use the \"max_features\" most common words as features)\n",
    "embed_len = 16 # How long is each embedding vector\n",
    "INDEX_FROM= 3 #Where does word indexing start\n",
    "maxlen = 234 #234 average lenght of review in set\n",
    "batch_size = 1024 ## 4096 causes accuracy problems due to gradiant problems\n",
    "                  ## 512 or 1024 has best results with time used and accuracy\n",
    "(x_traintext, y_trainvalue), (x_testtext, y_testvalue) = imdb.load_data(num_words=max_features, index_from=INDEX_FROM,skip_top=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 234.75892\n"
     ]
    }
   ],
   "source": [
    "number_of_tokens = [len(i) for i in x_traintext + x_testtext]\n",
    "print(\"Average length:\" ,np.mean(number_of_tokens) / 2) \n",
    "#dividing by two because of way I'm adding x_traintext and x_testtext together here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "##https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "word_to_id[\"<DEL>\"] = 3\n",
    "## 0-3 is for custom use index 4 is the first word\n",
    "id_to_word = {value:key for key,value in word_to_id.items()} ##tokenizer.word_index()\n",
    "inverse_map = dict(zip(word_to_id.values(), word_to_id.keys())) ##used in assessing vector distance in the embedding layer\n",
    "\n",
    "#Cut out 5000 reviews and their sentiment value\n",
    "x_remove_text = x_testtext[20000:]\n",
    "y_remove_sentiment = y_testvalue[20000:]\n",
    "x_testtext = np.delete(x_testtext, np.s_[20000:], axis = 0)\n",
    "y_testvalue = np.delete(y_testvalue, np.s_[20000:], axis = 0)\n",
    "\n",
    "# turn all sets into actual words\n",
    "x_trainfulltext = func.set_to_text(x_traintext,id_to_word)\n",
    "x_testfulltext = func.set_to_text(x_testtext,id_to_word)\n",
    "\n",
    "## x_all_fullremove_text has the reviews as is, if needed later for comparison\n",
    "x_all_fullremove_text = func.set_to_text(x_remove_text,id_to_word) \n",
    "\n",
    "x_fullremove_text = func.pos_tagger(x_all_fullremove_text)\n",
    "x_fullremove_text = func.text_to_id(x_fullremove_text,word_to_id)\n",
    "\n",
    "#All the pos_tagging is done before padding and cutting so the pos_tagger has full context\n",
    "x_traintext = sequence.pad_sequences(x_traintext, maxlen=maxlen)\n",
    "x_testtext = sequence.pad_sequences(x_testtext, maxlen=maxlen)\n",
    "x_remove_text = sequence.pad_sequences(x_fullremove_text, maxlen=maxlen)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\pumaska\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\pumaska\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\pumaska\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = Sequential()\\nmodel.add(Embedding(max_features, embed_len, input_length=maxlen,name=\"layer_embedding\"))\\nmodel.add(LSTM(32, return_sequences=True, recurrent_regularizer=l2(0.01)))\\nmodel.add(Dropout(0.3))\\nmodel.add(LSTM(16, recurrent_regularizer=l2(0.01)))\\nmodel.add(Dropout(0.3))\\nmodel.add(Dense(1, activation=\\'sigmoid\\'\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = load_model('model_1.h5')\n",
    "#Uncomment if you are training your own model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_len, input_length=maxlen,name=\"layer_embedding\"))\n",
    "model.add(LSTM(32, return_sequences=True, recurrent_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(16, recurrent_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmo = model.fit(x_traintext, y_trainvalue,\\n          batch_size=batch_size,\\n          epochs=6,\\n          validation_data=[x_testtext, y_testvalue])\\n\\npyplot.plot(mo.history[\\'loss\\'], label=\\'train\\')\\npyplot.plot(mo.history[\\'val_loss\\'], label=\\'test\\')\\npyplot.legend()\\npyplot.show()\\n#Save your own model\\n#model.save(\"model_2.h5\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment if you are training you own model\n",
    "\n",
    "mo = model.fit(x_traintext, y_trainvalue,\n",
    "          batch_size=batch_size,\n",
    "          epochs=6,\n",
    "          validation_data=[x_testtext, y_testvalue])\n",
    "\n",
    "pyplot.plot(mo.history['loss'], label='train')\n",
    "pyplot.plot(mo.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "#Save your own model\n",
    "#model.save(\"model_2.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base accuracy 0.8689\n",
      "Evaluation accuracy 0.65\n"
     ]
    }
   ],
   "source": [
    "#Evaluate model with full context\n",
    "testresults = model.evaluate(x_testtext, y_testvalue,verbose=0)\n",
    "print(\"Base accuracy\",testresults[1])\n",
    "#Evaluate model with word classes removed\n",
    "evaluate = model.evaluate(x_remove_text, y_remove_sentiment,verbose=0)\n",
    "print(\"Evaluation accuracy\",evaluate[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #EVERYTHING UNDER HERE IS NOT REALLY RELEVANT CURRENTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> <DEL> <DEL> <DEL> <UNK> <DEL> <DEL> us a <DEL> film a simple but <DEL> story <DEL> in a <DEL> paradise and the music can't <DEL> <DEL> stephen <UNK> <UNK> us <DEL> <DEL> and <DEL> <UNK> but why on earth <DEL> <DEL> <UNK> in this film i <DEL> myself that <DEL> question while <DEL> this movie of course her <DEL> accent <DEL> <DEL> <DEL> she's of <DEL> <UNK> <DEL> <DEL> in one scene <DEL> <DEL> at the end she <DEL> a <DEL> job after <DEL> <DEL> events after years she <DEL> <DEL> to <DEL> <DEL> and she <DEL> <DEL> the <DEL> face of we <DEL> to <DEL> during the <DEL> film <DEL> cage <DEL> <DEL> <DEL> in one of his <DEL> characters hurt <DEL> <DEL> <DEL> in his and the rest of the cast all <DEL> a <DEL> job so the <DEL> result <DEL> a movie that <DEL> <DEL> worth of <DEL> this <DEL> a <DEL> film that <DEL> <DEL> <DEL> <UNK> <DEL> <DEL> to <DEL> <DEL> <DEL> <DEL> think <DEL> <UNK> <DEL> work <DEL> no fan will <DEL> <DEL>\n",
      "Model Prediction: 0.06441891\n",
      "Actual Sentiment: 1\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb\n",
    "y_value = model.predict(x_remove_text[0:1000])\n",
    "y_value = y_value.T[0]\n",
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_value])\n",
    "cls_true = np.array(y_remove_sentiment[0:1000])\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]\n",
    "\n",
    "\n",
    "index = incorrect[2]\n",
    "print(len(incorrect))\n",
    "func.texter(x_remove_text,index,id_to_word)\n",
    "print(\"Model Prediction:\",y_value[index])\n",
    "print(\"Actual Sentiment:\",y_remove_sentiment[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sorted_words(word, metric='cosine'):\n",
    "    #https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/20_Natural_Language_Processing.ipynb\n",
    "    layer_embedding = model.get_layer('layer_embedding')\n",
    "    weights_embedding = layer_embedding.get_weights()[0]\n",
    "    \"\"\"\n",
    "    Print the words in the vocabulary sorted according to their\n",
    "    embedding-distance to the given word.\n",
    "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Get the token (i.e. integer ID) for the given word.\n",
    "    token = word_to_id[word]\n",
    "    \n",
    "    # Get the embedding for the given word. Note that the\n",
    "    # embedding-weight-matrix is indexed by the word-tokens\n",
    "    # which are integer IDs.\n",
    "    embedding = weights_embedding[token]\n",
    "    \n",
    "    # Calculate the distance between the embeddings for\n",
    "    # this word and all other words in the vocabulary.\n",
    "    distances = cdist(weights_embedding, [embedding],\n",
    "                      metric=metric).T[0]\n",
    "    \n",
    "    # Get an index sorted according to the embedding-distances.\n",
    "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
    "    sorted_index = np.argsort(distances)\n",
    "   \n",
    "    # Sort the embedding-distances.\n",
    "    sorted_distances = distances[sorted_index]\n",
    "    \n",
    "    # Sort all the words in the vocabulary according to their\n",
    "    # embedding-distance. This is a bit excessive because we\n",
    "    # will only print the top and bottom words.\n",
    "    sorted_words = [inverse_map[token] for token in sorted_index if token != 0]\n",
    "\n",
    "    # Helper-function for printing words and embedding-distances.\n",
    "    def _print_words(words, distances):\n",
    "        for word, distance in zip(words, distances):\n",
    "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
    "\n",
    "    # Number of words to print from the top and bottom of the list.\n",
    "    k = 10\n",
    "\n",
    "    print(\"Distance from '{0}':\".format(word))\n",
    "\n",
    "    # Print the words with smallest embedding-distance.\n",
    "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
    "\n",
    "    print(\"...\")\n",
    "\n",
    "    # Print the words with highest embedding-distance.\n",
    "    _print_words(sorted_words[-k:], sorted_distances[-k:])\n",
    "\n",
    "print_sorted_words('great', metric='cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}